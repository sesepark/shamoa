import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import time
import urllib3
import pdfplumber
import io
import olefile  # <--- [ì¶”ê°€] HWP íŒŒì‹±ìš©
import zlib     # <--- [ì¶”ê°€] HWP ì••ì¶• í•´ì œìš©
import struct   # <--- [ì¶”ê°€] ë°”ì´ë„ˆë¦¬ í•´ì„ìš©
import base64  # <--- [ì¶”ê°€] ì´ë¯¸ì§€ë¥¼ GPTì—ê²Œ ë³´ë‚´ê¸° ìœ„í•œ ë³€í™˜ ë„êµ¬
import sqlite3 # <--- [ì¶”ê°€] DB ì €ì¥ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬


# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==========================================
# [ì„¤ì •] API í‚¤ ì…ë ¥
# ==========================================
# [ë³€ê²½] GitHubì— ì˜¬ë¦´ ë•ŒëŠ” ì´ë ‡ê²Œ ë”°ì˜´í‘œ ì•ˆì„ ë¹„ì›Œë‘ì„¸ìš”
API_KEY = "" 

client = OpenAI(api_key=API_KEY)

# [ì¶”ê°€] ë´‡ ì°¨ë‹¨ ë°©ì§€ìš© í—¤ë” (ë¸Œë¼ìš°ì €ì¸ ì²™ ì†ì„)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# ==========================================
# [ì„¤ì •] í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ ëª©ë¡
# ==========================================
SITES = [
    {"name": "ì„œìš¸ëŒ€ OIA", "url": "https://oia.snu.ac.kr/notice-all?combine=&page=0"},
    {"name": "ì„œìš¸ëŒ€ OIA", "url": "https://oia.snu.ac.kr/notice-all?combine=&page=1"},
    {"name": "ì„œìš¸ëŒ€ OIA", "url": "https://oia.snu.ac.kr/notice-all?combine=&page=2"},
    {"name": "ììœ ì „ê³µí•™ë¶€", "url": "https://cls.snu.ac.kr/notice/?pageid=1&mod=list"},
    {"name": "ììœ ì „ê³µí•™ë¶€", "url": "https://cls.snu.ac.kr/notice/?pageid=2&mod=list"},
    {"name": "ì„œìš¸ëŒ€ SR", "url": "https://snusr.snu.ac.kr/community/notice?page=1"},
    {"name": "ì„œìš¸ëŒ€ SR", "url": "https://snusr.snu.ac.kr/community/notice?page=2"},
    {"name": "ì¸ë¬¸ëŒ€í•™", "url": "https://humanities.snu.ac.kr/community/notice"},
    {"name": "ê³µê³¼ëŒ€í•™", "url": "https://eng.snu.ac.kr/snu/bbs/BMSR00004/list.do?menuNo=200176"},
    {"name": "ìì—°ê³¼í•™ëŒ€", "url": "https://science.snu.ac.kr/news/announcement"},
    {"name": "ê²½ì˜ëŒ€í•™", "url": "https://cba.snu.ac.kr/newsroom/notice?sc=y"},
    {"name": "ë†ì—…ìƒëª…ê³¼í•™ëŒ€", "url": "https://cals.snu.ac.kr/board/notice"},
    {"name": "ì‚¬íšŒê³¼í•™ëŒ€", "url": "https://social.snu.ac.kr/%ea%b3%b5%ec%a7%80%ec%82%ac%ed%95%ad/"},
    {"name": "ì‚¬ë²”ëŒ€í•™", "url": "https://edu.snu.ac.kr/category/board_17_gn_ldca7if5_20201130072915/"},
    {"name": "ìŒì•…ëŒ€í•™", "url": "https://music.snu.ac.kr/notice"},
    {"name": "ìˆ˜ì˜ê³¼ëŒ€í•™", "url": "https://vet.snu.ac.kr/category/board-3-BL-8Piv9u51-20211029154329/"},
    {"name": "ìƒí™œê³¼í•™ëŒ€", "url": "https://che.snu.ac.kr/category/board-35-GN-EKIrl47t-20210226142951/"},
    {"name": "ê°„í˜¸ëŒ€í•™", "url": "https://nursing.snu.ac.kr/board/notice"},
    {"name": "ì•½í•™ëŒ€í•™", "url": "https://snupharm.snu.ac.kr/%EA%B3%B5%EC%A7%80%EC%82%AC%ED%95%AD/"},
    {"name": "ì¹˜ì˜í•™ëŒ€í•™ì›", "url": "https://dentistry.snu.ac.kr/fnt/nac/selectNoticeList.do?bbsId=BBS_0000000000001"},
    {"name": "ì˜ê³¼ëŒ€í•™", "url": "https://medicine.snu.ac.kr/fnt/nac/selectNoticeList.do?bbsId=BBSMSTR_000000000001"},
    {"name": "í•™ë¶€ëŒ€í•™", "url": "https://snuc.snu.ac.kr/%ea%b3%b5%ec%a7%80%ec%82%ac%ed%95%ad/?pageid=1&mod=list"},
    {"name": "í•™ë¶€ëŒ€í•™", "url": "https://snuc.snu.ac.kr/%ea%b3%b5%ec%a7%80%ec%82%ac%ed%95%ad/?pageid=2&mod=list"}
]

EXCLUDE_KEYWORDS = [
    "Scholarship", "scholarship", "ì¥í•™ê¸ˆ", "êµí™˜í•™ìƒ", 
    "Exchange", "ë“±ë¡ê¸ˆ", "ìˆ˜ê°•ì‹ ì²­", "ì¡¸ì—…", "í•™ìœ„","ì¥í•™","ìˆ˜ì‹œ","ì •ì‹œ","í˜„ì—­ë³‘","ë³‘ì—­","êµ°ë³µë¬´",
    "ê·¼ë¡œ", "ì˜ˆë¹„êµ°", "íœ´í•™", "ë³µí•™", "ëŒ€ì¶œ","ê¸°ìˆ™ì‚¬","ì…í•™", "ì±„ìš©", "ì¸í„´", "ì±„ìš©", "ì·¨ì—…","ì „ì‹œíšŒ","ê³„ì ˆìˆ˜ì—…","ê³„ì ˆí•™ê¸°"
    "LNL", "ì‹ ì…ìƒ", "í¸ì…", "êµë‚´í™œë™", "ë™ì•„ë¦¬", "í•™ìƒíšŒ", "ì´í•™ìƒíšŒ","ì¡¸ì—…ìƒ","ë™ë¬¸","ë™ì°½íšŒ","ìˆ˜í—˜ìƒ","ë©´ì ‘","ë…¼ë¬¸","ì´ìˆ˜ê·œì •","ë°•ì‚¬í•™ìœ„", "ì„ì‚¬í•™ìœ„"
    "ì…”í‹€ë²„ìŠ¤","êµí†µ","ì£¼ì°¨","ì£¼ì°¨ì¥","ê°•ì˜í‰ê°€","ì‹œí—˜ê¸°ê°„",
    "í•™ìœ„ë³µ", "ì±„ìš© ê³µê³ ", "ê³ ì‚¬ì¥", "ë§Œì¡±ë„ ì¡°ì‚¬","ì…ì£¼ì","ì—°ê±´í•™ìƒìƒí™œê´€","ì¸ì‹¤","ì„œì—°ì¬","êµ­ê°€ì‹œí—˜","LnL", "ë„ì„œê´€", "êµì› ì´ˆë¹™", "ë‹¤ì „ê³µ", "ë³µìˆ˜ì „ê³µ", "ë¶€ì „ê³µ"
]

def encode_image_to_base64(image_url):
    try:
        # headers=HEADERS ì¶”ê°€
        response = requests.get(image_url, headers=HEADERS, verify=False, timeout=5)
        if response.status_code == 200:
            return base64.b64encode(response.content).decode('utf-8')
    except:
        return None
    return None

def extract_text_from_hwp(hwp_url):
    """
    [ì‹ ê·œ ì¶”ê°€] HWP íŒŒì¼ì˜ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    try:
        response = requests.get(hwp_url, headers=HEADERS, verify=False, timeout=10)
        f = io.BytesIO(response.content)
        
        try:
            ole = olefile.OleFileIO(f)
        except:
            return "" 

        text = ""
        dirs = ole.listdir()
        sections = [d for d in dirs if d[0] == "BodyText"]
        
        # ì„¹ì…˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬ (Section0, Section1...)
        sections.sort(key=lambda x: int(x[1].replace('Section', '')))

        for section in sections:
            try:
                stream = ole.openstream(section)
                data = stream.read()
                
                # HWPëŠ” ë‚´ìš©ì„ zlibìœ¼ë¡œ ì••ì¶•í•´ì„œ ì €ì¥í•¨ -> ì••ì¶• í•´ì œ
                unpacked_data = zlib.decompress(data, -15)
                
                decoded_text = unpacked_data.decode('utf-16-le', errors='ignore')
                
                # (ì‹¤ì œ HWP ë°”ì´ë„ˆë¦¬ êµ¬ì¡°ë¥¼ ì™„ë²½íˆ íŒŒì‹±í•˜ë ¤ë©´ ë³µì¡í•˜ë¯€ë¡œ, í…ìŠ¤íŠ¸ ë¤í”„ ë°©ì‹ ì‚¬ìš©)
                # í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ë§Œ ì¶”ì¶œ (ê°„ì´ ë°©ì‹)
                clean_text = ""
                for char in decoded_text:
                    if char.isprintable() or char in ['\n', '\t', ' ']:
                        clean_text += char
                
                text += clean_text + "\n"

            except Exception:
                continue
                
        return text.strip()

    except Exception as e:
        return f""


def extract_text_from_pdf(pdf_url):
    """ [ê¸°ì¡´ ìœ ì§€ + í˜ì´ì§€ ì „ì²´ ì½ê¸° ì ìš©ë¨] """
    try:
        response = requests.get(pdf_url, verify=False, timeout=10)
        f = io.BytesIO(response.content)
        text = ""
        with pdfplumber.open(f) as pdf:
            for page in pdf.pages: 
                extracted = page.extract_text()
                if extracted:
                    text += extracted + "\n"
        return text.strip()
    except Exception:
        return ""


def get_full_content(url):
    try:
        response = requests.get(url, headers=HEADERS, verify=False, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
        content_div = (
            soup.select_one('.board-view-con') or 
            soup.select_one('.view-content') or 
            soup.select_one('.bbs_view') or
            soup.select_one('.con_area') or
            soup.select_one('#board_view') or
            soup.select_one('.bo_v_con')
        )
        web_text = content_div.get_text(strip=True)[:1000] if content_div else ""
        search_area = content_div if content_div else soup

        # 2. ì²¨ë¶€íŒŒì¼(PDF, HWP) ì²˜ë¦¬
        files = search_area.select('a')
        file_text_list = []
        for f in files:
            href = f.get('href', '')
            text = f.get_text(strip=True)
            if 'privacy' in href.lower(): continue
            
            full_url = href
            if not href.startswith('http'):
                base_url = "/".join(url.split('/')[:3])
                full_url = base_url + href if href.startswith('/') else base_url + '/' + href
            
            extracted = ""
            if full_url.lower().endswith('.pdf'):
                extracted = extract_text_from_pdf(full_url)
            elif full_url.lower().endswith('.hwp'):
                extracted = extract_text_from_hwp(full_url)
            
            if len(extracted) > 10:
                file_text_list.append(f"--- [ì²¨ë¶€íŒŒì¼: {text}] ---\n{extracted}")

        # 3. [ì¶”ê°€ëœ ë¶€ë¶„] ë³¸ë¬¸ ì´ë¯¸ì§€(jpg, png) ì°¾ê¸°
        images = search_area.select('img')
        image_base64_list = []
        
        for img in images:
            src = img.get('src', '')
            # ë¡œê³ ë‚˜ ì•„ì´ì½˜ ê°™ì€ ì“¸ë°ì—†ëŠ” ì´ë¯¸ì§€ ì œì™¸
            if not src or 'button' in src or 'icon' in src or 'logo' in src or 'common' in src: continue
            
            img_url = src
            if not src.startswith('http'):
                base_url = "/".join(url.split('/')[:3])
                img_url = base_url + src if src.startswith('/') else base_url + '/' + src
            
            # ì´ë¯¸ì§€ ë³€í™˜í•´ì„œ ë¦¬ìŠ¤íŠ¸ì— ë‹´ê¸°
            base64_str = encode_image_to_base64(img_url)
            if base64_str:
                image_base64_list.append(base64_str)
                if len(image_base64_list) >= 3: break # ìµœëŒ€ 3ì¥ë§Œ (ë¹„ìš© ì ˆì•½)

        full_text_content = f"[ì›¹ ë³¸ë¬¸]\n{web_text}\n\n" + "\n".join(file_text_list)
        
        # í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°™ì´ ë°˜í™˜
        return full_text_content, image_base64_list

    except Exception:
        return "ë‚´ìš© í™•ì¸ ë¶ˆê°€", []






def init_db():
    conn = sqlite3.connect('snu_programs.db')
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS programs
                 (link TEXT PRIMARY KEY, 
                  site_name TEXT, 
                  title TEXT, 
                  status TEXT, 
                  target TEXT, 
                  reason TEXT, 
                  period TEXT,  
                  content TEXT, 
                  crawled_at DATETIME DEFAULT CURRENT_TIMESTAMP)''') 
    conn.commit()
    conn.close()





def save_to_db(site_name, title, link, status, target, reason, period, content):
    try:
        conn = sqlite3.connect('snu_programs.db')
        c = conn.cursor()
        # period ì»¬ëŸ¼ ì¶”ê°€ë¨
        c.execute('''INSERT OR REPLACE INTO programs 
                     (link, site_name, title, status, target, reason, period, content)
                     VALUES (?, ?, ?, ?, ?, ?, ?, ?)''', 
                     (link, site_name, title, status, target, reason, period, content))
        conn.commit()
        conn.close()
    except Exception as e:
        print(f"   ğŸ’¾ DB ì €ì¥ ì‹¤íŒ¨: {e}")





def analyze_program(title, content, images=[]): # <--- ì¸ìì— images ì¶”ê°€ë¨
    """ LLMì—ê²Œ íŒë‹¨ ìš”ì²­ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€) """
    
    
    prompt_text = f"""
    You are an AI assistant for Seoul National University students.
    Your goal is to identify **"Short-term Overseas Programs"** from university notices.

    [Selection Criteria]
    1. **YES (Include)**:
       - Clearly stated as a short-term overseas program (Summer/Winter school, Field trip, Cultural exchange).
       - Duration: Less than 1 month.
       - Location: Outside Korea.
       
    2. **CHECK (Needs Confirmation)**:
       - **Ambiguous**: The title looks like an overseas program, but the content is missing, too short, or unclear.
       - **Image Only**: If the text is empty but there are attached images, check the images for details.
       - **Benefit of Doubt**: If you are 60-90% sure it's relevant but miss key details (like specific dates), choose [CHECK].

    3. **NO (Exclude)**:
       - Clearly Domestic (Korea).
       - Long-term Exchange (Semester/Year).
       - Pure Job/Internship/Scholarship without cultural element.

    [Input Data]
    Title: {title}
    Content Summary: {content}

    [Output Instructions]
    - **Response Format**:
      íŒë‹¨: [YES] or [NO] or [CHECK]
      ëŒ€ìƒ: (Target audience)
      ê¸°ê°„: (Program Date, e.g., "2024.01.15 ~ 01.30" or "ê³µì§€ ì°¸ì¡°")
      ìš”ì•½: (Please summarize the provided text into 1 to 3 polite Korean sentences describing what the program is. Focus on the core nature and purpose of the activity. Do not include specific details about dates or target audience.)
    """

    # [ë³€ê²½ëœ ë¶€ë¶„] GPTì—ê²Œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ê°™ì´ ë³´ë‚´ëŠ” ê·œê²©
    messages = [
        {"role": "system", "content": "You are a helpful assistant. Follow the output format strictly."},
        {
            "role": "user", 
            "content": [{"type": "text", "text": prompt_text}] # í…ìŠ¤íŠ¸ ì¶”ê°€
        }
    ]

    # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë©”ì‹œì§€ì— ì´ë¯¸ì§€ ì¶”ê°€
    for base64_img in images:
        messages[1]["content"].append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{base64_img}"
            }
        })
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o", # Visionì€ gpt-4o í•„ìˆ˜
            messages=messages,
            temperature=0
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"íŒë‹¨: [CHECK]\nëŒ€ìƒ: í™•ì¸ ë¶ˆê°€\nì´ìœ : API ì—ëŸ¬ ({e})"





def find_best_title_link(row, base_url):
    """ ê°€ì¥ ì œëª©ìŠ¤ëŸ¬ìš´ ë§í¬ ì°¾ê¸° """
    links = row.select('a')
    best_link = None
    best_text = ""

    for link in links:
        text = link.get_text(strip=True)
        href = link.get('href', '')
        
        if len(text) < 4: continue
        if any(ext in href.lower() for ext in ['.pdf', '.hwp', '.zip', 'download']): continue
            
        if len(text) > len(best_text):
            best_text = text
            best_link = href

    if not best_link: return None, None

    if not best_link.startswith('http'):
        domain = "/".join(base_url.split('/')[:3])
        if best_link.startswith('/'):
            best_link = domain + best_link
        else:
            best_link = domain + '/' + best_link

    return best_text, best_link





def crawl_site(site_info):
    name = site_info['name']
    url = site_info['url']
    
    print(f"\n>>> ğŸ« [{name}] í¬ë¡¤ë§ ì‹œì‘...")
    
    try:
        # headers=HEADERS ì¶”ê°€ (ë´‡ ì°¨ë‹¨ íšŒí”¼)
        response = requests.get(url, headers=HEADERS, verify=False, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        rows = []
        
        # [ê²Œì‹œíŒ êµ¬ì¡° ì°¾ê¸°]
        rows = soup.select('tbody tr')
        if not rows:
            rows = soup.select('table tr')
            rows = [r for r in rows if r.select('td')]
        if not rows:
            rows = (
                soup.select('li.list_item') or 
                soup.select('.board-list li') or 
                soup.select('.list_board li') or
                soup.select('.notice_list li') or
                soup.select('ul.board_list li')
            )

        print(f"   ã„´ ê²Œì‹œê¸€ {len(rows)}ê°œ ìŠ¤ìº” ì‹œì‘")
        
        processed = 0
        for row in rows:
            if processed >= 20: break 
            
            title, link = find_best_title_link(row, url)
            if not title: continue

            # [1ë‹¨ê³„: ì œëª© í•„í„°ë§]
            if any(k in title for k in EXCLUDE_KEYWORDS) or any(k in title.lower() for k in ["scholarship", "exchange"]):
                print(f"   ğŸš« [ì¦‰ì‹œ ì œì™¸] {title}")
                processed += 1
                continue

            print(f"   ğŸ” ë¶„ì„ ì¤‘: {title}")
            
            full_content, images = get_full_content(link)        
            result = analyze_program(title, full_content, images) 
            
            # [ê²°ê³¼ íŒŒì‹± ìˆ˜ì •]
            lines = result.split('\n')
            status = "NO"
            target = "í™•ì¸ í•„ìš”"
            reason = ""     # ê¸°ë³¸ê°’
            period = "ê³µì§€ ì°¸ì¡°"

            for line in lines:
                if line.startswith("íŒë‹¨:"):
                    if "YES" in line: status = "YES"
                    elif "CHECK" in line: status = "CHECK"
                
                # 'ëŒ€ìƒ' íŒŒì‹± (ê³µë°±ì´ë‚˜ * ë“± ì œê±°)
                if "ëŒ€ìƒ:" in line:
                    target = line.split("ëŒ€ìƒ:")[-1].strip().strip("*")
                
                # 'ê¸°ê°„' íŒŒì‹±
                if "ê¸°ê°„:" in line:
                    period = line.split("ê¸°ê°„:")[-1].strip().strip("*")
                
                # [ì—¬ê¸°ê°€ í•µì‹¬ ìˆ˜ì •] GPTëŠ” 'ìš”ì•½:'ì´ë¼ê³  ë§í•˜ë¯€ë¡œ 'ìš”ì•½'ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.
                if "ìš”ì•½:" in line:
                    reason = line.split("ìš”ì•½:")[-1].strip().strip("*")
                # í˜¹ì‹œ 'ì´ìœ :'ë¼ê³  ë§í–ˆì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ëŒ€ë¹„
                elif "ì´ìœ :" in line:
                    reason = line.split("ì´ìœ :")[-1].strip().strip("*")

            # [ê²°ê³¼ ì¶œë ¥]
            if status == "YES":
                print(f"   ğŸ‰ [ë°œê²¬!] {title}")
                print(f"       ğŸ‘‰ ëŒ€ìƒ: {target}")
                print(f"       ğŸ‘‰ ì´ìœ : {reason}")
            elif status == "CHECK":
                print(f"   ğŸ¤” [í™•ì¸ í•„ìš”] {title}")
                print(f"       ğŸ‘‰ ì‚¬ìœ : {reason}")
            else:
                print(f"       âŒ [íƒˆë½] {reason}")

            # ========================================================
            # [í•µì‹¬] DBì— ì €ì¥í•˜ê¸° (YES, CHECK, NO ëª¨ë‘ ì €ì¥)
            # ========================================================
            save_to_db(name, title, link, status, target, reason, period, full_content)
            # ========================================================
            
            processed += 1
            time.sleep(1)

    except Exception as e:
        print(f"   ğŸš¨ ì—ëŸ¬ ë°œìƒ: {e}")






if __name__ == "__main__":
    init_db()  # <--- í”„ë¡œê·¸ë¨ ì‹œì‘ ì „ DB ì´ˆê¸°í™”
    print("=== [PDF/Vision ë¶„ì„ + DB ì €ì¥] ì„œìš¸ëŒ€ í•´ì™¸ í”„ë¡œê·¸ë¨ í¬ë¡¤ëŸ¬ ===")
    
    for site in SITES:
        crawl_site(site)
        print("-" * 60)